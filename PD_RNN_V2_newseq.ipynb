{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"PD_RNN_V2_newseq.ipynb","provenance":[{"file_id":"15bxbkmiGjE7pxR3TXLsIZo9sGitOekSG","timestamp":1628068081952}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3q4xzS8QHRRv"},"source":["# Generative Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJQ46wLHrCKt","executionInfo":{"status":"ok","timestamp":1631105763302,"user_tz":300,"elapsed":1941,"user":{"displayName":"Suvarcha Narayan","photoUrl":"","userId":"15093882350046911644"}},"outputId":"1779bb0e-162a-44c0-a2ba-98503429dd6f"},"source":["!git clone https://github.com/MauriceR71/UniRep.git\n","%cd UniRep\n","!mv petase_seqs.txt petase_seqs_0.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'UniRep'...\n","remote: Enumerating objects: 164, done.\u001b[K\n","remote: Total 164 (delta 0), reused 0 (delta 0), pack-reused 164\u001b[K\n","Receiving objects: 100% (164/164), 908.15 KiB | 5.16 MiB/s, done.\n","Resolving deltas: 100% (79/79), done.\n","/content/UniRep\n"]}]},{"cell_type":"code","metadata":{"id":"_2u9Wqllr8Z9"},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","import numpy as np\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XPv7wLMNDJV"},"source":["## Instability"]},{"cell_type":"code","metadata":{"id":"3TXAJ8X8sk0w"},"source":["AMINO_ACIDS = ['W', 'C', 'M', 'H', 'Y', 'F', 'Q', 'N', 'I', 'R', 'D', 'P', 'T',\n","               'K', 'E', 'V', 'S', 'G', 'A', 'L']\n","\n","# Returns a dictionary that maps pairs of amino acids to their instability\n","# score according to ProtParam's Instability Index\n","def construct_diwv():\n","  diwv_dict = {}\n","  matrix = []\n","\n","  # diwv.csv is the matrix of instability values found in\n","  # Guruprasad, K., Reddy, B.V.B. and Pandit, M.W. (1990)\n","  with open(\"./data/diwv.csv\", \"r\") as f:\n","    for line in f.readlines():\n","      scores = line.split(\",\")\n","      matrix.append([float(s) for s in scores])\n","  for i in range(20):\n","    subdict = {}\n","    for j in range(20):\n","      subdict[AMINO_ACIDS[j]] = matrix[i][j]\n","    diwv_dict[AMINO_ACIDS[i]] = subdict\n","  return diwv_dict\n","  \n","diwv_dict = construct_diwv()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPd4vOqCNY75"},"source":["## Load Discriminative Model"]},{"cell_type":"code","metadata":{"id":"ZBpd1u9fssjK"},"source":["import pickle\n","from sklearn.ensemble import AdaBoostRegressor\n","\n","regr = 0\n","# Loads the regression model that performed best on cross-validation loss\n","# If you wish, you can replace <regr> with your own model\n","# with open(\"AdaBoostRegressor_18_2.pkl\", \"rb\") as f:\n","#     regr = pickle.load(f)\n","\n","# Returns True iff protein sequence passes preset filters\n","# You can also add additional filters as heuristics,\n","#     e.g. secondary structure, alignment scores, conserved regions, etc.\n","def passes_filters(seq, min_len=270, max_len=310,\n","                   max_instability_index=40,\n","                   max_conserved_residue_penalty=20):\n","\n","  # controls sequence length\n","  if len(seq) <= 270 or len(seq) >= 310:\n","    return False\n","  \n","  # controls sequence stability (ProtParam)\n","  score = 0\n","  for i in range(len(seq)-1):\n","    score += diwv_dict[seq[i]][seq[i+1]]\n","    score = 10.0/len(seq) * score\n","  if score > 40.0:\n","    return False\n","  else: \n","    return True\n","\n","  # ensures optimal sequence function using oracles\n","  # unirep_vec = b.get_rep(seq)\n","  # if regr.predict(unirep_vec.reshape(1,-1)) > 2.5:\n","  #   return True\n","  # return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lSjgRdBonfRF"},"source":["## RNN Sequence Generator"]},{"cell_type":"code","metadata":{"id":"Rchy7gFZneSC"},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdegJMoL6K2c","executionInfo":{"status":"ok","timestamp":1631105788484,"user_tz":300,"elapsed":107,"user":{"displayName":"Suvarcha Narayan","photoUrl":"","userId":"15093882350046911644"}},"outputId":"c3e2ca53-5229-4ee8-93f7-abc575e89683"},"source":["seq_length=100\n","batch_size=2 \n","BUFFER_SIZE=10000 \n","epochs=30\n","embedding_dim=256\n","rnn_units=1024\n","num_generate=290\n","temperature=0.1\n","seed=u\"MNFPRASRLMQAAVL\"\n","path_to_file=\"./sequences-final.txt\"\n"," \n","# the full string of all sequences\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","text = text.replace('\\n', '')\n","text = text.replace('\\r', '')\n","text = text.replace(' ', '')\n","print ('Length of text: {} characters'.format(len(text)))\n","\n","# a collection of the unique letters in the file, should be 22\n","vocab = sorted(set(text))\n","print('{} unique characters'.format(len(vocab)))\n","\n","print(vocab)\n","# print(text[5580:5586])\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of text: 21149 characters\n","20 unique characters\n","['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n"]}]},{"cell_type":"code","metadata":{"id":"ih2BknxCF4QL"},"source":["# A wrapper that trains an RNN on the sequences in <path_to_file>\n","#  default parameters are provided\n","# Returns: A list of strings, each of which is a generated sequence that\n","#          passes the filters specified in the passes_filters() function\n","def run_RNN(seq_length=100, batch_size=2, BUFFER_SIZE=10000, epochs=30,\n","            embedding_dim=256, rnn_units=1024, num_generate=290,\n","            temperature=0.1, seed=u\"MNFPRASRLMQAAVL\",\n","            path_to_file=\"./sequences-final.txt\", num_seqs = 5):\n","  \n","  # the full string of all sequences\n","  text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","  text = text.replace('\\n', '')\n","  text = text.replace('\\r', '')\n","  text = text.replace(' ', '')\n","  print ('Length of text: {} characters'.format(len(text)))\n","\n","  # a collection of the unique letters in the file, should be 22\n","  vocab = sorted(set(text))\n","  print('{} unique characters'.format(len(vocab)))\n","\n","  # unique mapping from letters to integers\n","  char2idx = {u:i for i, u in enumerate(vocab)}\n","\n","  # unique mapping from integers to letters\n","  idx2char = np.array(vocab)\n","\n","  # the integer representation of the string\n","  text_as_int = np.array([char2idx[c] for c in text])\n","  print('{} ---map---> {}'.format(text[:13], text_as_int[:13]))\n","\n","  examples_per_epoch = len(text)//(seq_length+1)\n","\n","  inputs = []\n","  outputs = []\n","  # cuts the input file into slices of length==seq_length\n","  # each slice is an input\n","  # the same slice but one letter shifted to the right is its desired output\n","  for i in range(seq_length, len(text), seq_length):\n","    inputs.append(text_as_int[i-seq_length:i])\n","    outputs.append(text_as_int[i-seq_length+1:i+1])\n","  inputs = inputs[:-1]\n","  outputs = outputs[:-1]\n","\n","  # groups the data as (input_i, output_i) pairs, so shuffling them preserves\n","  # their correspondance\n","  dataset = list(zip(inputs, outputs))\n","  random.shuffle(dataset)\n","  dataset = list(zip(*dataset))\n","  X, y = np.array(dataset[0]), np.array(dataset[1])\n","  print(X.shape)\n","  print(y.shape)\n","\n","  # batch size must be able to divide into the number of training examples\n","  batch_size = 2\n","  vocab_size = len(vocab)\n","\n","  def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","    model = tf.keras.Sequential([\n","      tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                                batch_input_shape = [batch_size, seq_length]),\n","      tf.keras.layers.GRU(rnn_units,\n","                          return_sequences=True,\n","                          stateful=True,\n","                          recurrent_initializer='glorot_uniform'), \n","      # tf.keras.layers.Flatten(batch_input_shape = [batch_size, seq_length]),\n","      # tf.keras.layers.Reshape(batch_size, seq_length), \n","      tf.keras.layers.Dense(vocab_size, activation = 'relu')\n","    ])\n","    return model\n","\n","  model = build_model(\n","    vocab_size = len(vocab),\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units,\n","    batch_size=batch_size)\n","  \n","  model.summary()\n","\n","  def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","  model.compile(optimizer='adam', loss=loss)\n","\n","  # Setup checkpoints\n","  checkpoint_dir = './training_checkpoints'\n","  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","      filepath=checkpoint_prefix,\n","      save_weights_only=True)\n","\n","  # Train the model\n","  history = model.fit(X, y, epochs=epochs, callbacks=[checkpoint_callback], batch_size=batch_size)\n","\n","  # Load the latest model\n","  tf.train.latest_checkpoint(checkpoint_dir)\n","\n","  # Prepare for sequence generation\n","  model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","  model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","  model.build(tf.TensorShape([1, None]))\n","\n","  # Returns one string generated by the RNN <model>, based on a short seed\n","  # given as <start_string>\n","  def generate_text(model, start_string):\n","    input_eval = [char2idx[s] for s in start_string]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    text_generated = []\n","\n","    temperature = 1.0\n","\n","    model.reset_states()\n","    for i in range(num_generate):\n","        predictions = model(input_eval)\n","\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        predictions = predictions / temperature\n","        \n","        # samples the alphabet based on <predictions> interpreted as a vector\n","        # of probabilities per letter\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        text_generated.append(idx2char[predicted_id])\n","\n","    return (start_string + ''.join(text_generated))\n","\n","\n","  valid_seqs = []\n","  while len(valid_seqs) < num_seqs:\n","    blabble = generate_text(model, seed)\n","    # print(blabble)\n","    # print(len(valid_seqs))\n","    # seq = blabble.split(\"\\n\") #[0]\n","    seq = blabble\n","    # print(len(seq))\n","    if not passes_filters(seq): #seq.strip()\n","      continue\n","    else:\n","      valid_seqs.append(seq)\n","      \n","  return valid_seqs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mX43xDq-LRzb"},"source":["# Final Pipeline"]},{"cell_type":"code","metadata":{"id":"MVujIecSs96k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631106070215,"user_tz":300,"elapsed":263420,"user":{"displayName":"Suvarcha Narayan","photoUrl":"","userId":"15093882350046911644"}},"outputId":"e9095c61-c642-4748-8b62-d2dc8c468118"},"source":["valid_seqs = run_RNN(num_seqs = 3)\n","print(\"Valid Sequences = \", len(valid_seqs))\n","\n","with open(\"output.txt\", \"w\") as f:\n","  for seq in valid_seqs:\n","    print(\"seq: \", seq)\n","    f.write(seq + \"\\n\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of text: 21149 characters\n","20 unique characters\n","MNFPRASRLMQAA ---map---> [10 11  4 12 14  0 15 14  9 10 13  0  0]\n","(210, 100)\n","(210, 100)\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (2, 100, 256)             5120      \n","_________________________________________________________________\n","gru (GRU)                    (2, 100, 1024)            3938304   \n","_________________________________________________________________\n","dense (Dense)                (2, 100, 20)              20500     \n","=================================================================\n","Total params: 3,963,924\n","Trainable params: 3,963,924\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/30\n","105/105 [==============================] - 15s 77ms/step - loss: 2.4005\n","Epoch 2/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.7502\n","Epoch 3/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.3559\n","Epoch 4/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.2890\n","Epoch 5/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.2485\n","Epoch 6/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.2257\n","Epoch 7/30\n","105/105 [==============================] - 8s 75ms/step - loss: 0.2194\n","Epoch 8/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.2023\n","Epoch 9/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1970\n","Epoch 10/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1863\n","Epoch 11/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1768\n","Epoch 12/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1788\n","Epoch 13/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1758\n","Epoch 14/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1645\n","Epoch 15/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1624\n","Epoch 16/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1581\n","Epoch 17/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1605\n","Epoch 18/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1521\n","Epoch 19/30\n","105/105 [==============================] - 8s 75ms/step - loss: 0.1540\n","Epoch 20/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1544\n","Epoch 21/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1542\n","Epoch 22/30\n","105/105 [==============================] - 8s 77ms/step - loss: 0.1560\n","Epoch 23/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1525\n","Epoch 24/30\n","105/105 [==============================] - 8s 77ms/step - loss: 0.1516\n","Epoch 25/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1504\n","Epoch 26/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1445\n","Epoch 27/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1456\n","Epoch 28/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1518\n","Epoch 29/30\n","105/105 [==============================] - 8s 77ms/step - loss: 0.1477\n","Epoch 30/30\n","105/105 [==============================] - 8s 76ms/step - loss: 0.1453\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n","WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n","WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n","Valid Sequences =  3\n","seq:  MNFPRASRLMQAAVLGGLMAVSAAATAQTNPYARGPNPTAASLEASAGPFTVRSFTVSRPSGYGAGTVYYPTNAGGTVGAIAIVPGYTARQSSIKWWGPRLASHGFVVITIDTNSTLDQPSSRSSQQMAALRQVASLNGTSSSPIYGKVDTARMGVMGWSMGGGGSLISAANNPSLKAAAPQAPVDSSTNFSSVTVPTLIFACENDSIAPVNSSALPIYDSMSRNAYQFLEAMSPGYTADASSLAWLGRRLASHGFVVLVINTNSRFDYPDSRASQLSAALNKARMDPYQRGPNPTRSALTADGP\n","seq:  MNFPRASRLMQAAVLGGLMAVSAAATAQTNPYARGPNPTAASLEASAGPFTVRSFTVSRPSGYGAGTVYYPTNAGGTVGAIAIVPGYTARQSSIKWWGPRLASHGFVVITIDTNSTLDQPSSRSSQQMAALRQVASLNGTSSSPIYGKVDTARMGVMGWSMGGGGSLISAANNPSLKAAAPQAPSDSRTNNSSALPIYDSMSRNAPQFLEQVSTFRQFPNLPSTTPINGGSHSCANSGNSNQALIGKKGVAWMKRFMDNDTRYSTFACENPNSTAVSDFRTANCSMNFPRASRLMQAAVLGGLMA\n","seq:  MNFPRASRLMQAAVLGGLMAVSAAATAQTNPYARGPNPTAASLEASAGPFTVRSFTVSRPSGYGAGTVYYPTNAGGTVGAIAIVPGYTARQSSIKWWGPRLASHGFVVITIDTNSTLDQPSSRSSQQMAALRQVASLNGTSSSPIYGKVDTARMGVMGWSMGGGGSLISAANNPSLKAAAPQAPEDSSTNFSSVTVPTLIFACENDSIAPVNSSALPIYDSMSRNAFQFLEINGGSHCCANSGNSNQALIGKKGVAWMKRFMDNDTRYSTFACENPNSTRVSDFRTANCSMNFPRASRLMQAAVL\n"]}]}]}